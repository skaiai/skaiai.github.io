<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Skaiai</title>
		<description>Welcome！</description>
		<link>http://localhost:4000/skaiai.github.io</link>
		<atom:link href="http://localhost:4000/skaiai.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>어메이징타이 (서초)</title>
				<description>&lt;p&gt;last update: 2018-09-30&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;어메이징타이를 가보았다! 
첫 번째 가보고 너무 맛있어서 바로 다음날 또 갔을 정도로 너무 맛있었다.
왜 지금 알았을까 하는 후회가 밀려올만큼…&lt;/p&gt;

&lt;p&gt;일단 내가 거기서 맛 본 메뉴는 총 세가지다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;똠양쌀국수&lt;/li&gt;
  &lt;li&gt;그린커리&lt;/li&gt;
  &lt;li&gt;소고기 쌀국수&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 세 가지는 점심특선메뉴에 해당되기 때문에 누구라도 부담없이 즐길 수 있을 것 같다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180930/amazing_tai_menu.jpeg&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;첫째날에 똠양쌀국수와 그린커리를 먹었는데, 왠걸! 이렇게 똠양꿍과 비슷한 똠양쌀국수는 처음 맛본다. 저번 포스팅에서도 얘기했다시피 보통 식당에서 나오는 똠양쌀국수는 똠양꿍과 맛이 좀 달라서 실망을 하기 마련인데 말이다.. 그린커리도 로컬한 맛으로 아주 맛있었다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180930/amazing_tai_ttomyam.jpeg&quot; width=&quot;400&quot; /&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180930/amazing_tai_green_curry.jpeg&quot; width=&quot;400&quot; /&gt;
음.. 음식 나오자마자 눈 뒤집혀서 먹는 바람에 사진을 먹는 도중 찍었다….. ㅠㅠㅠㅠㅠ
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;다음날 점심에 한달음에 달려가 소고기 쌀국수를 시켰다. 음식에서부터 시작해서 인테리어까지 태국의 로컬함이 많이 묻어났기 때문에 쌀국수가 내가 좋아하는 태국식 쌀국수일꺼라는 확신이 들었기 때문이다. 난 베트남 쌀국수도 물론 좋지만 태국식 쌀국수가 훨씬 맛있다. 근데 한국에서는 베트남 쌀국수가 훨씬 쉽게 찾아볼 수 있지.. ㅠㅠ 
&lt;br /&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180930/amazing_tai_rice_noodle.jpeg&quot; width=&quot;400&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;암튼 나의 예상은 틀리지 않았다. 내가 원하던 그 맛. 태국식 쌀국수. 헤헤. 국물까지 다 먹어서 밑바닥이 들어나도록 후루룩 먹었더니 거기 태국 아저씨가 날 쳐다보셨다. 민망…&lt;/p&gt;

&lt;p&gt;회사 사람들 꼬셔서 점심시간에 일주일에 한번씩 오기로 했다. ^^ 기대가 되는 태국 맛집이다.&lt;/p&gt;

&lt;p&gt;내 점수 &lt;strong&gt;5/5&lt;/strong&gt;&lt;/p&gt;
</description>
				<pubDate>Sun, 30 Sep 2018 00:00:00 +0900</pubDate>
				<link>http://localhost:4000/skaiai.github.io/food/2018/09/30/amazing_tai.html</link>
				<guid isPermaLink="true">http://localhost:4000/skaiai.github.io/food/2018/09/30/amazing_tai.html</guid>
			</item>
		
			<item>
				<title>Sequence-to-Sequence Models (2): Attention is all you need</title>
				<description>&lt;!--last update: 2018-09-20 &lt;br/&gt;&lt;br/&gt;--&gt;

&lt;p&gt;Last update: 2018-09-26&lt;br /&gt;
Previous Post: &lt;a href=&quot;/skaiai.github.io/sequence-to-sequence/2018/09/20/seq2seq.html&quot;&gt;Sequence-to-Sequence models (1)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Attention is all you need&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;2017 by Google Brain/Research, University of Toronto&lt;/li&gt;
  &lt;li&gt;description: attention operations are introduced in encoder and decoder nets both instead of rnn or cnn&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
</description>
				<pubDate>Wed, 26 Sep 2018 00:00:00 +0900</pubDate>
				<link>http://localhost:4000/skaiai.github.io/sequence-to-sequence/2018/09/26/transformer.html</link>
				<guid isPermaLink="true">http://localhost:4000/skaiai.github.io/sequence-to-sequence/2018/09/26/transformer.html</guid>
			</item>
		
			<item>
				<title>메콩타이 (강남교보타워점)</title>
				<description>&lt;p&gt;last update: 2018-09-26&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;길었던 연휴를 보내며.. 나 자신에게 힘을 내라고 똠양꿍 선물을 선사하기로 했다.&lt;/p&gt;

&lt;p&gt;집과 정말 가까이 있지만 지나치기만 하고 한번도 가보지 않았던 메콩타이를 가기로 마음먹음. 혹시나 연휴 때문에 안열까봐 모바일로 확인해보니 다행히도 11시 30분부터 오픈이다! 쉬지 않고 일하시는 분들 화이팅 ㅠㅠ&lt;/p&gt;

&lt;p&gt;자, 두근거리는 마음으로 메뉴를 펴보았다.
내가 원하는 건 똠양 시리즈들이므로 메뉴의 두 부분을 제외한 곳은 아웃오브안중.
&lt;br /&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180926/TtomyangKkung_menu.jpeg&quot; width=&quot;400&quot; /&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180926/TtomyamNoodle_menu.jpeg&quot; width=&quot;400&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 똠양꿍과 똠양쌀국수가 두 개 있을 때에는 고민을 하게 된다. 내가 좋아하는 건 똠양꿍. 하지만.. 똠양쌀국수는 가격적인 측면에서 보통 우위에 있다. 이 집만 해도 똠양꿍은 14,800 + 면 2,000 = 16,800 원이지만 똠양쌀국수는 11,900 원이다. 흑흑 나한테 왜이래 ㅠㅠ 얇은 지갑이 생각났기 때문에 똠양쌀국수를 선택했다..
&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;여기서 잠깐! 
똠양꿍과 똠양쌀국수는 다르다. 전자는 우리가 익히 알고 있는, 세계3 대 스프 중 하나인 그거다.. 어떻게 설명해야되지;; 만약 면과 함께 먹고 싶으면 '면추가', 밥과 먹고 싶으면 '밥추가' 옵션을 골라 먹는다. 후자인 똠양쌀국수는 클래식 똠양꿍과는 맛이 좀 다르다. 쌀국수 베이스에 똠양 소스를 추가한 형태기 때문에 똠양꿍의 그 진하고 깊은 맛을 따라가지는 못한다.. 지극히 개인적인 생각
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180926/TtomyamNoodle.jpeg&quot; width=&quot;500&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;짜잔! 똠양쌀국수가 나왔다.&lt;br /&gt;
고수 많이 달라고 했더니 수북히 주셔서 엄청나게 만족함 ㅋㅋ 어떤 곳은 고수 달라고 하면 너무 조금 줘서 아껴먹느라 정신 없는데 말이다.
아무튼 본격적으로 맛을 보았다.&lt;br /&gt;&lt;br /&gt;
맛 점수는 &lt;strong&gt;3/5&lt;/strong&gt; 이다. 이건 똠양쌀국수라는 메뉴를 감안하고 매긴 점수다. (내가 똠양쌀국수를 똠양꿍보다 별로 안 좋아하니까 너무 낮은 점수를 피하기 위해)&lt;br /&gt;&lt;br /&gt;
난 &lt;strong&gt;신맛&lt;/strong&gt;을 매우 좋아하는 편이다. 특히 똠양에서의 신맛은 나를 너무 행복하게 만든다. 하지만 여기서 맛 본 신맛은 조화롭지 못하고 혼자 튀는 느낌이었다. 그리고 똠양의 신맛이라기보다는 참치김치찌개의 신맛같았다. 뭔소리야 하는 분들도 계시겠지만 내가 받은 느낌은 그랬다. ㅋㅋㅋ 나에게 맛있는 신맛은 매우 중요하기 때문에 조금 아쉬웠다. 그래도 많이 제공해주신 고수로 그 신맛을 덮으며 먹었더니 먹을만했다. &lt;br /&gt;&lt;br /&gt;
다음번에는 돈을 좀 더 들여서 ‘똠양꿍 + 면추가’ 메뉴를 시도해봐야겠다.&lt;/p&gt;

</description>
				<pubDate>Wed, 26 Sep 2018 00:00:00 +0900</pubDate>
				<link>http://localhost:4000/skaiai.github.io/food/2018/09/26/mekong_tai.html</link>
				<guid isPermaLink="true">http://localhost:4000/skaiai.github.io/food/2018/09/26/mekong_tai.html</guid>
			</item>
		
			<item>
				<title>Sequence-to-Sequence Models (1)</title>
				<description>&lt;!--last update: 2018-09-20 &lt;br/&gt;&lt;br/&gt;--&gt;

&lt;p&gt;Last update: 2018-09-26&lt;br /&gt;
Next Post: &lt;a href=&quot;/skaiai.github.io/sequence-to-sequence/2018/09/26/transformer.html&quot;&gt;Sequence-to-Sequence models (2)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;!-- &gt; End-to-end approach to speech recognition is appealing to us, as it turns separate models such as AM (acoustic), PM (pronunciation), LM (language) of conventional hybrid ASR system into one single neural network model.&lt;br/&gt;&lt;br/&gt;Seq-to-seq models have been used for this purpose and I'd like to review them.&lt;br/&gt; --&gt;

&lt;p&gt;End-to-end approach to speech recognition is appealing to us, as it turns separate models such as AM (acoustic), PM (pronunciation), LM (language) of conventional hybrid ASR system into one single neural network model.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Seq-to-seq models have been used for this purpose and I’d like to review them.&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;seq2seq-models&quot;&gt;Seq2Seq Models&lt;/h2&gt;
&lt;h4 id=&quot;1-connectionist-temporal-classification-ctc&quot;&gt;1. Connectionist temporal classification (&lt;strong&gt;CTC&lt;/strong&gt;)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~graves/icml_2006.pdf&quot;&gt;Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;when: 2006 by Alex Graves&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2-recurrent-neural-netowrk-transducer-rnn-t&quot;&gt;2. Recurrent neural netowrk Transducer (&lt;strong&gt;RNN-T&lt;/strong&gt;)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1211.3711.pdf&quot;&gt;Sequence transduction with recurrent neural networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;when: Nov 2012 by Alex Graves&lt;/li&gt;
  &lt;li&gt;description: extension of CTC (it augments CTC w/ prediction network)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-attention-based-model&quot;&gt;3. Attention-based Model&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;LAS&lt;/strong&gt; model (non-streaming input)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.01211.pdf&quot;&gt;Listen, Attend and Spell&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;when: Aug 2015 by CMU, Google Brain&lt;/li&gt;
      &lt;li&gt;description: encoder, decoder and attention btw them. non-streaming input&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nueral Transducer&lt;/strong&gt; (streaming input)
    &lt;ul&gt;
      &lt;li&gt;improved: &lt;a href=&quot;https://pdfs.semanticscholar.org/9409/ce44b3c6b1b55479f3ff0f87f4e7c52f227a.pdf&quot;&gt;Improving the performance of online neural transducer models&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;Dec 2017 by Google&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;org: &lt;a href=&quot;http://bengio.abracadoudou.com/cv/publications/pdf/jaitly_2016_nips.pdf&quot;&gt;An online sequence-to-sequence model using partial conditioning&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;when: 2016 by Google Brain / DeepMind, OpenAI&lt;/li&gt;
          &lt;li&gt;description: streaming input&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-rnn-t-w-attention&quot;&gt;4. &lt;strong&gt;RNN-T w/ Attention&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/6cc6/8e8adf34b580f3f37d1bd267ee701974edde.pdf&quot;&gt;A comparison of sequence-to-sequence models for speech recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;2017 by Google, NVIDIA&lt;/li&gt;
  &lt;li&gt;description: modification to the RNN transducer replacing the prediction network w/ attention-based decoder entwork used in LAS&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-transformer&quot;&gt;5. &lt;strong&gt;Transformer&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Attention is all you need&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;2017 by Google Brain/Research, University of Toronto&lt;/li&gt;
  &lt;li&gt;description: attention operations are introduced in encoder and decoder nets both instead of rnn or cnn&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;Next post will be discussing Transformer model and how to apply it to ASR task.&lt;/p&gt;
</description>
				<pubDate>Thu, 20 Sep 2018 00:00:00 +0900</pubDate>
				<link>http://localhost:4000/skaiai.github.io/sequence-to-sequence/2018/09/20/seq2seq.html</link>
				<guid isPermaLink="true">http://localhost:4000/skaiai.github.io/sequence-to-sequence/2018/09/20/seq2seq.html</guid>
			</item>
		
			<item>
				<title>[Paper] State-of-the-art speech recognition with sequence-to-sequence models</title>
				<description>&lt;p&gt;last update: 2018-09-20  &lt;br /&gt;
paper: &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/46687.pdf&quot;&gt;State-of-the-art speech recognition with sequence-to-sequence models (Feb 2018)&lt;/a&gt;
&lt;!--[example pic](/skaiai.github.io/assets/images/post_180920/header.png= 250x)--&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180920/header.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;
&lt;p&gt;improved version of LAS model&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;structural side
    &lt;ul&gt;
      &lt;li&gt;words instead of graphemes&lt;/li&gt;
      &lt;li&gt;multi-head attention instead of a single-head one&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;optimization side
    &lt;ul&gt;
      &lt;li&gt;synchronous training&lt;/li&gt;
      &lt;li&gt;scheduled sampling&lt;/li&gt;
      &lt;li&gt;label smoothing&lt;/li&gt;
      &lt;li&gt;minimum word error rate optimization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Thu, 20 Sep 2018 00:00:00 +0900</pubDate>
				<link>http://localhost:4000/skaiai.github.io/speech-recognition/2018/09/20/paper-sota_sr_seq2seq.html</link>
				<guid isPermaLink="true">http://localhost:4000/skaiai.github.io/speech-recognition/2018/09/20/paper-sota_sr_seq2seq.html</guid>
			</item>
		
			<item>
				<title>[Paper] Listen, Attend and Spell</title>
				<description>&lt;p&gt;Last update: 2018-09-20  &lt;br /&gt;
Paper: &lt;a href=&quot;https://arxiv.org/pdf/1508.01211.pdf&quot;&gt;Listen, Attend and Spell (Aug 2015)&lt;/a&gt;
&lt;img name=&quot;header&quot; img=&quot;&quot; src=&quot;/skaiai.github.io/assets/images/post_180920/header_las.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;

</description>
				<pubDate>Thu, 20 Sep 2018 00:00:00 +0900</pubDate>
				<link>http://localhost:4000/skaiai.github.io/speech-recognition/2018/09/20/paper-LAS.html</link>
				<guid isPermaLink="true">http://localhost:4000/skaiai.github.io/speech-recognition/2018/09/20/paper-LAS.html</guid>
			</item>
		
			<item>
				<title>ReadMe</title>
				<description>&lt;h1 id=&quot;license&quot;&gt;License&lt;/h1&gt;
&lt;p&gt;The content of this theme is distributed and licensed under a
&lt;img src=&quot;/skaiai.github.io/assets/images/cc_by_88x31.png&quot; alt=&quot;License Badge&quot; /&gt;
&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;&gt;Creative Commons Attribution 4.0 License&lt;/a&gt;
    This license lets others distribute, remix, tweak, and build upon your work,
    even commercially, as long as they credit you for the original creation. This
    is the most accommodating of licenses offered. Recommended for maximum
    dissemination and use of licensed materials.&lt;/p&gt;

&lt;h1 id=&quot;thanks&quot;&gt;Thanks&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;[Bootstrap][bootstrap-url]&lt;/li&gt;
  &lt;li&gt;[Jekyll Clean Theme][Jekyll-Clean-Theme-url]&lt;/li&gt;
  &lt;li&gt;[Theme Maker][xixia-url]
[bootstrap-url]: http://getbootstrap.com/
[Jekyll-Clean-Theme-url]: https://github.com/scotte/jekyll-clean
[xixia-url]: http://xixia.info/&lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Sun, 02 Sep 2018 00:00:00 +0900</pubDate>
				<link>http://localhost:4000/skaiai.github.io/other/2018/09/02/README.html</link>
				<guid isPermaLink="true">http://localhost:4000/skaiai.github.io/other/2018/09/02/README.html</guid>
			</item>
		
	</channel>
</rss>
